import argparse
import sys
import os
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from joblib import dump, load


def run_train(public_dir, model_dir):
    os.makedirs(model_dir, exist_ok=True)
    
    # Load training data from JSON
    train_path = os.path.join(public_dir, 'train_data', 'train.json')
    df = pd.read_json(train_path, lines=True)

    # Split features and label
    
    X = df.drop('two_year_recid', axis=1)
    y = df['two_year_recid']

    # Identify categorical and numerical columns
    cat_features = X.select_dtypes(include=['object', 'category']).columns.tolist()
    num_features = X.select_dtypes(include=[np.number]).columns.tolist()

    # Preprocessing pipeline
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', SimpleImputer(strategy='mean'), num_features),
            ('cat', Pipeline([
                ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),
                ('imputer', SimpleImputer(strategy='most_frequent'))
            ]), cat_features)
        ]
    )

    # Full feature pipeline
    X_processed = preprocessor.fit_transform(X)

    # Model training
    # model = LogisticRegression(max_iter=1000)
    model = KNeighborsClassifier()
    model.fit(X_processed, y)

    # Save model and preprocessor
    dump(model, os.path.join(model_dir, 'trained_model.joblib'))
    dump(preprocessor, os.path.join(model_dir, 'preprocessor.joblib'))


def run_predict(model_dir, test_input_dir, output_path):
    # Load model and preprocessor
    model = load(os.path.join(model_dir, 'trained_model.joblib'))
    preprocessor = load(os.path.join(model_dir, '   '))
    # Load test data from JSON
    test_path = os.path.join(test_input_dir, 'test.json')
    df_test = pd.read_json(test_path, lines=True)
    # Transform test data
    X_test = preprocessor.transform(df_test)
    preds = model.predict(X_test)
    # Save predictions with proper column name
    pd.DataFrame({'two_year_recid': preds}).to_json(output_path, orient='records', lines=True)

def main():
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest='command')

    # Train command
    parser_train = subparsers.add_parser('train')
    parser_train.add_argument('--public_dir', type=str, default='./')
    parser_train.add_argument('--model_dir', type=str, default ='./')

    # Predict command
    parser_predict = subparsers.add_parser('predict')
    parser_predict.add_argument('--model_dir', type=str, default='./')
    parser_predict.add_argument('--test_input_dir', type=str, default='./test_data')
    parser_predict.add_argument('--output_path', type=str, default='./predict.json')

    args = parser.parse_args()

    if args.command == 'train':
        run_train(args.public_dir, args.model_dir)
    elif args.command == 'predict':
        run_predict(args.model_dir, args.test_input_dir, args.output_path)
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()
